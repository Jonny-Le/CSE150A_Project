{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b07rsOCnDMKA"
      },
      "outputs": [],
      "source": [
        "# Beer-Liking Prediction Pipeline (Downloadable .ipynb)\n",
        "\n",
        "*This notebook follows the Milestone 2 structure: Data Prep, Modeling, Evaluation.*\n",
        "\n",
        "```python\n",
        "# %matplotlib inline\n",
        "```\n",
        "\n",
        "*This notebook follows the Milestone 2 structure: Data Prep, Modeling, Evaluation.*\n",
        "\n",
        "```python\n",
        "# %matplotlib inline\n",
        "```\n",
        " (Colab Script)\n",
        "\n",
        "# 1. Setup & Imports\n",
        "# ------------------\n",
        "# Install pgmpy if needed\n",
        "# !pip install pgmpy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "\n",
        "\n",
        "# 2. Data Loading & Preprocessing\n",
        "# --------------------------------\n",
        "def load_and_clean_data(beers_path, ratings_path, like_threshold=3.5):\n",
        "    \"\"\"\n",
        "    Load beer attributes and ratings, merge, drop NAs, binarize ratings.\n",
        "    \"\"\"\n",
        "    beers = pd.read_csv(beers_path)\n",
        "    ratings = pd.read_csv(ratings_path)\n",
        "    df = pd.merge(beers, ratings, on='beer_id', how='inner')\n",
        "    df = df.dropna(subset=['abv', 'ibu', 'ounces', 'style', 'rating_score'])\n",
        "    df['Like'] = (df['rating_score'] >= like_threshold).astype(int)\n",
        "    return df\n",
        "\n",
        "\n",
        "# 3. Exploratory Analysis\n",
        "# -----------------------\n",
        "def explore_variables(df):\n",
        "    \"\"\"\n",
        "    Display info, stats, and basic plots.\n",
        "    \"\"\"\n",
        "    print(df.info())\n",
        "    print(df.describe())\n",
        "    df[['abv','ibu','ounces']].hist(bins=30, figsize=(12,4))\n",
        "    top_styles = df['style'].value_counts().nlargest(10)\n",
        "    top_styles.plot.bar(figsize=(8,4), title='Top 10 Beer Styles')\n",
        "\n",
        "\n",
        "# 4. Train/Test Split\n",
        "# -------------------\n",
        "def split_data(df, test_size=0.2, random_state=42):\n",
        "    X = df[['abv','ibu','ounces','style']]\n",
        "    y = df['Like']\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "\n",
        "# 5. Model Structure & Parameter Learning\n",
        "# ---------------------------------------\n",
        "model = BayesianModel([\n",
        "    ('abv', 'Like'),\n",
        "    ('ibu', 'Like'),\n",
        "    ('ounces', 'Like'),\n",
        "    ('style', 'Like')\n",
        "])\n",
        "\n",
        "def learn_parameters(model, df):\n",
        "    \"\"\"Fit the BayesianModel using Maximum Likelihood Estimation with Laplace smoothing.\"\"\"\n",
        "    model.fit(df, estimator=MaximumLikelihoodEstimator, prior_type='laplace')\n",
        "    return model\n",
        "\n",
        "\n",
        "# 6. Training\n",
        "# -----------\n",
        "# Example usage:\n",
        "# df = load_and_clean_data('beers.csv', 'ratings.csv')\n",
        "# X_train, X_test, y_train, y_test = split_data(df)\n",
        "# train_df = X_train.copy(); train_df['Like'] = y_train\n",
        "# trained_model = learn_parameters(model, train_df)\n",
        "\n",
        "\n",
        "# 7. Evaluation\n",
        "# -------------\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    return acc, prec, rec, f1, cm\n",
        "\n",
        "\n",
        "# 8. Future Work\n",
        "# --------------\n",
        "# - Compare to scikit-learn's CategoricalNB/GaussianNB\n",
        "# - Structure learning: Tree-Augmented NB\n",
        "# - Add contextual features (weather, mood)\n",
        "# - Build a simple UI in Streamlit\n",
        "\n",
        "# To download this as a .ipynb file, run the following Python code in a Colab cell:\n",
        "# ```python\n",
        "# import nbformat as nbf\n",
        "#\n",
        "# # Read this script's text\n",
        "# with open('/content/beer_pipeline_colab.py') as f:\n",
        "#     script = f.read().splitlines()\n",
        "#\n",
        "# nb = nbf.v4.new_notebook()\n",
        "# cells = []\n",
        "# for line in script:\n",
        "#     if line.startswith('# %% [markdown]'):\n",
        "#         cells.append(nbf.v4.new_markdown_cell('\n",
        "'.join(script[script.index(line)+1: script.index(line)+1])))\n",
        "#     # For brevity, assume script is preformatted into notebook cells\n",
        "# # Instead, you can manually save this notebook via File > Download .ipynb\n",
        "# ```\n"
      ]
    }
  ]
}